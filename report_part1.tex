\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{float}

\title{ESE 3060 Final Project -- Part 1\\
\large Effect of Activation Functions on CIFAR-10 Training Speed}
\author{Chafic Akhras}
\date{December 8, 2025}

\begin{document}
\maketitle

\section{Introduction}
The \texttt{airbench94} CIFAR-10 training script achieves 94\% accuracy in approximately 3.8 seconds on an NVIDIA A100 GPU. This project investigates whether replacing the default GELU activation with ReLU\textsuperscript{2} (squared ReLU) can further improve training speed. While ReLU\textsuperscript{2} has been proposed for Transformers (e.g., NanoGPT), its efficacy in highly optimized Convolutional Neural Networks (CNNs) for CIFAR-10 has not been systematically established. I demonstrate a statistically significant 1.5\% speedup with no loss in accuracy.

\section{Hypothesis}
I hypothesize that replacing GELU with ReLU\textsuperscript{2} will speed up training because:
\begin{enumerate}
    \item \textbf{Computational cost}: GELU requires expensive error function computations. ReLU\textsuperscript{2} uses simple clamping and squaring: $\text{ReLU}^2(x) = (\max(0, x))^2$.
    \item \textbf{Gradient properties}: Unlike standard ReLU, ReLU\textsuperscript{2} has a continuous derivative at zero, providing smoother gradients which may aid convergence compared to the cheaper but discontinuous standard ReLU.
\end{enumerate}

\section{Method}
I modified the training script (see \texttt{airbench94\_activation.py}) to support configurable activation functions. I compared three variants: (1) GELU (Baseline), (2) ReLU\textsuperscript{2} (Main), and (3) ReLU (Ablation).
Experiments were conducted on an NVIDIA A100-SXM4-80GB GPU. To ensure statistical significance, I performed \textbf{25 runs per variant}. 
I reported the mean test accuracy (with test-time augmentation) and total wall-clock training time. Full logs and reproduction scripts are included in the submission repository.

\section{Results}
As shown in Table \ref{tab:results}, ReLU\textsuperscript{2} achieves a statistically significant speedup over GELU while maintaining equivalent accuracy. Standard ReLU is faster but suffers a statistically significant accuracy drop.

\begin{table}[h]
\centering
\caption{Activation Function Comparison (25 runs). ReLU\textsuperscript{2} provides the best trade-off.}
\begin{tabular}{lcccccc}
\toprule
Activation & Acc (\%) & Std & Time (s) & Speedup & $p$-value (time) & Significant? \\
\midrule
\textbf{GELU} & \textbf{94.04} & \textbf{0.16} & \textbf{4.65} & - & - & Baseline \\
ReLU\textsuperscript{2} & 94.02 & 0.15 & 4.58 & \textbf{+1.5\%} & \textbf{0.023} & \textbf{YES} \\
ReLU & 93.95 & 0.18 & 4.55 & +2.2\% & 0.003 & YES* \\
Swish & 94.01 & 0.17 & 4.67 & -0.4\% & 0.498 & NO \\
\bottomrule
\end{tabular}
\label{tab:results}
\footnotesize{*ReLU shows significant speedup but also significant accuracy degradation ($p=0.036$).}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/activation_combined.pdf}
\caption{Comparison of Accuracy and Training Time. ReLU\textsuperscript{2} (green) reduces training time compared to GELU (orange) without the accuracy penalty seen in standard ReLU.}
\label{fig:activation_combined}
\end{figure}

\section{Discussion and Conclusion}
My results confirm that ReLU\textsuperscript{2} is a superior alternative to GELU for this workload, offering a **1.5\% speedup** ($p=0.023$) with no compromise in model quality.

\textbf{Novelty:} While ReLU\textsuperscript{2} has been suggested for Transformer-based LLMs (like NanoGPT), this work demonstrates its transferability to CNN-based image classification. I find that the "smoothness" property of ReLU\textsuperscript{2} is crucial: it outperforms standard ReLU in accuracy, likely because standard ReLU's discontinuous gradient hinders optimization in this aggressive training regime. This provides empirical evidence that activation function choice remains a relevant optimization vector even in sub-5-second training scripts.

\newpage
\appendix
\section{Appendix}

\subsection{Implementation Details}
The core modification to \texttt{airbench94\_activation.py} involves the definition of the squared ReLU class and a factory function to select activations dynamically. This allows for fair, controlled ablation studies within the same codebase.

\begin{verbatim}
class ReLUSquared(nn.Module):
    """
    ReLU^2 activation: F.relu(x).square()
    Provides smoother gradients than standard ReLU while being
    computationally cheaper than GELU.
    """
    def forward(self, x):
        return F.relu(x).square()

def get_activation(name):
    if name == 'gelu':
        return nn.GELU()
    elif name == 'relu':
        return nn.ReLU()
    elif name == 'relu_squared':
        return ReLUSquared()
    elif name == 'swish':
        return nn.SiLU()
\end{verbatim}

\subsection{Additional Figures}
I included additional visualizations of the experimental results below.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/activation_speedup.pdf}
\caption{Relative training speedup. ReLU\textsuperscript{2} provides a consistent 1.5\% improvement over the GELU baseline.}
\label{fig:speedup}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/activation_accuracy.pdf}
\caption{Distribution of test accuracies across 25 runs. All variants achieve similar final accuracy, confirming that the speedup does not come at the cost of model performance.}
\label{fig:acc_dist}
\end{figure}

\subsection{Compute Usage}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{compute_usage.png}
\end{figure}

\end{document}
